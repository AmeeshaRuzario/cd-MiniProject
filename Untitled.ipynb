{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa3770c-d06b-4a1d-93af-96e2100c58ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens are:\n",
      "\n",
      "['int', 'main()', 'begin', 'int', 'L[10]', ';', 'int', 'maxval', '=', 'L[0]', ';', 'for', 'i', '=', '1', 'to', 'n', '-', '1', 'do', 'if', 'L[i]', '>', 'maxval', 'maxval', '=', 'L[i]', ';', 'endif', 'endfor', 'return', '(', 'maxval', ')', 'End']\n",
      "============================================================================================\n",
      "\n",
      "Calculated firsts: \n",
      "first(S) => {'int'}\n",
      "first(T) => {'int'}\n",
      "first(M) => {'main()'}\n",
      "first(B) => {'begin'}\n",
      "first(D) => {'End'}\n",
      "first(A) => {'int'}\n",
      "first(E) => {'int'}\n",
      "first(F) => {'int'}\n",
      "first(G) => {'for'}\n",
      "first(C) => {'i'}\n",
      "first(W) => {'if'}\n",
      "first(P) => {'maxval'}\n",
      "first(Q) => {'endif'}\n",
      "first(R) => {'endfor'}\n",
      "first(X) => {'return'}\n",
      "\n",
      "Calculated follows: \n",
      "follow(S) => {'$'}\n",
      "follow(T) => {'maxval', 'main()', 'L[10]'}\n",
      "follow(M) => {'begin'}\n",
      "follow(B) => {'int'}\n",
      "follow(D) => {'$'}\n",
      "follow(A) => {'End'}\n",
      "follow(E) => {'int'}\n",
      "follow(F) => {'for'}\n",
      "follow(G) => {'if'}\n",
      "follow(C) => {'do'}\n",
      "follow(W) => {'return'}\n",
      "follow(P) => {'endif'}\n",
      "follow(Q) => {'endfor'}\n",
      "follow(R) => {'return'}\n",
      "follow(X) => {'End'}\n",
      "\n",
      "\n",
      "============================================================================================\n",
      "Firsts and Follow Result table\n",
      "╒═════════╤════════════╤═══════════════════════════════╕\n",
      "│ Non-T   │ FIRST      │ FOLLOW                        │\n",
      "╞═════════╪════════════╪═══════════════════════════════╡\n",
      "│ S       │ {'int'}    │ {'$'}                         │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ T       │ {'int'}    │ {'maxval', 'main()', 'L[10]'} │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ M       │ {'main()'} │ {'begin'}                     │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ B       │ {'begin'}  │ {'int'}                       │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ D       │ {'End'}    │ {'$'}                         │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ A       │ {'int'}    │ {'End'}                       │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ E       │ {'int'}    │ {'int'}                       │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ F       │ {'int'}    │ {'for'}                       │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ G       │ {'for'}    │ {'if'}                        │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ C       │ {'i'}      │ {'do'}                        │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ W       │ {'if'}     │ {'return'}                    │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ P       │ {'maxval'} │ {'endif'}                     │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ Q       │ {'endif'}  │ {'endfor'}                    │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ R       │ {'endfor'} │ {'return'}                    │\n",
      "├─────────┼────────────┼───────────────────────────────┤\n",
      "│ X       │ {'return'} │ {'End'}                       │\n",
      "╘═════════╧════════════╧═══════════════════════════════╛\n",
      "\n",
      "\n",
      "============================================================================================\n",
      "Validate String:\n",
      "\n",
      "int main()\n",
      "begin\n",
      "int L[10] ;\n",
      "int maxval = L[0] ; \n",
      "for i = 1 to n - 1 do\n",
      "if L[i] > maxval\n",
      "maxval = L[i] ;\n",
      "endif\n",
      "endfor\n",
      "return ( maxval )\n",
      "End\n",
      "\n",
      "\n",
      "Look into the parsing.txt file for the parsing steps\n",
      "\n",
      "\n",
      "Valid String!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================================================================================\n",
    "\n",
    "'''\n",
    "The function takes a dictionary rulesDiction as input, where keys are non-terminal symbols (left-hand  sides of production rules), and values are lists of right-hand sides for each non-terminal.\n",
    "'''\n",
    "\n",
    "def removeLeftRecursion(rulesDiction):\n",
    "    store = {}\n",
    "    for lhs in rulesDiction:\n",
    "        # alphaRules will store the rules with left recursion.\n",
    "        # betaRules will store the rules without left recursion.\n",
    "        # allrhs is the list of right-hand sides for the current non-terminal lhs\n",
    "        alphaRules = []\n",
    "        betaRules = []\n",
    "        allrhs = rulesDiction[lhs]\n",
    "\n",
    "        # Separate into 2 groups those with left recursion and those without\n",
    "        for subrhs in allrhs:\n",
    "            if subrhs[0] == lhs:\n",
    "                alphaRules.append(subrhs[1:])\n",
    "            else:\n",
    "                betaRules.append(subrhs)\n",
    "\n",
    "        ''' If there are rules with left recursion (alphaRules is not empty), it creates a new non-terminal symbol (lhs_) to replace the left-recursive rules. The loop ensures that the new symbol doesn't already exist in the original grammar or in the temporary storage (store).'''\n",
    "\n",
    "        if len(alphaRules) != 0:\n",
    "            lhs_ = lhs + \"'\"\n",
    "            while lhs_ in rulesDiction.keys() or lhs_ in store.keys():\n",
    "                lhs_ += \"'\"\n",
    "\n",
    "            # For each rule in betaRules, it appends the new non-terminal lhs_ to the end of the rule and updates the original non-terminal's rules with the modified betaRules.\n",
    "\n",
    "            for b in range(0, len(betaRules)):\n",
    "                betaRules[b].append(lhs_)\n",
    "            rulesDiction[lhs] = betaRules\n",
    "\n",
    "            for a in range(0, len(alphaRules)):\n",
    "                alphaRules[a].append(lhs_)\n",
    "            alphaRules.append(['#'])\n",
    "            store[lhs_] = alphaRules\n",
    "    for left in store:\n",
    "        # Result of left recursion will be stored in temp storage store\n",
    "        rulesDiction[left] = store[left]\n",
    "    return rulesDiction\n",
    "\n",
    "# =================================================================================================\n",
    "\n",
    "def LeftFactoring(rulesDiction):\n",
    "    # This dictionary will store the left-factored grammar rules.\n",
    "    newDict = {}\n",
    "    for lhs in rulesDiction:\n",
    "        # Group right-hand sides (rhs) based on the first terminal/non-terminal in each:\n",
    "        allrhs = rulesDiction[lhs]\n",
    "        temp = dict()\n",
    "        for subrhs in allrhs:\n",
    "            if subrhs[0] not in list(temp.keys()):\n",
    "                temp[subrhs[0]] = [subrhs]\n",
    "            else:\n",
    "                temp[subrhs[0]].append(subrhs)\n",
    "        # Process each group:\n",
    "        new_rule = []\n",
    "        tempo_dict = {}\n",
    "        for term_key in temp:\n",
    "            allStartingWithTermKey = temp[term_key]\n",
    "            # If a group has more than one rule, perform left factoring:\n",
    "            if len(allStartingWithTermKey) > 1:\n",
    "                lhs_ = lhs + \"'\"\n",
    "                while lhs_ in rulesDiction.keys() or lhs_ in tempo_dict.keys():\n",
    "                    lhs_ += \"'\"\n",
    "                new_rule.append([term_key, lhs_])\n",
    "                ex_rules = []\n",
    "                for g in temp[term_key]:\n",
    "                    ex_rules.append(g[1:])\n",
    "                tempo_dict[lhs_] = ex_rules\n",
    "            # If a group has only one rule, keep it unchanged:\n",
    "            else:\n",
    "                new_rule.append(allStartingWithTermKey[0])\n",
    "        # Update the newDict with the left-factored rules:\n",
    "        newDict[lhs] = new_rule\n",
    "        for key in tempo_dict:\n",
    "         newDict[key] = tempo_dict[key]\n",
    "    return newDict\n",
    "\n",
    "# ==================================================================================================\n",
    "\n",
    "def first(rule):\n",
    "    global rules, nonterm_userdef, term_userdef, diction, firsts\n",
    "    if len(rule) != 0 and (rule is not None):\n",
    "        if rule[0] in term_userdef:\n",
    "            return rule[0]\n",
    "        elif rule[0] == '#': # For epsilon\n",
    "            return '#'\n",
    "        \n",
    "    # If the first symbol is a non-terminal, recursively calculate the FIRST set for the corresponding  right-hand side rules in the grammar.\n",
    "    if len(rule) != 0:\n",
    "            if rule[0] in list(diction.keys()):\n",
    "                fres = []\n",
    "                rhs_rules = diction[rule[0]]\n",
    "                for itr in rhs_rules:\n",
    "                    indivRes = first(itr)\n",
    "                    if type(indivRes) is list:\n",
    "                        for i in indivRes:\n",
    "                            fres.append(i)\n",
    "                    else:\n",
    "                        fres.append(indivRes)\n",
    "\n",
    "                '''\n",
    "                If the FIRST set of the non-terminal contains epsilon ('#'), remove it from the set. If the remaining symbols in the rule can derive epsilon, add epsilon back to the set. \n",
    "                '''\n",
    "\n",
    "                if '#' not in fres:\n",
    "                        return fres\n",
    "                else:\n",
    "                        newList = []\n",
    "                        fres.remove('#')\n",
    "                        if len(rule) > 1:\n",
    "                            ansNew = first(rule[1:])\n",
    "                            if ansNew != None:\n",
    "                                if type(ansNew) is list:\n",
    "                                    newList = fres + ansNew\n",
    "                                else:\n",
    "                                    newList = fres + [ansNew]\n",
    "                            else:\n",
    "                                newList = fres\n",
    "                        fres.append('#')\n",
    "                        return newList\n",
    "                \n",
    "# ==================================================================================================\n",
    "\n",
    "def follow(nt):\n",
    "    global start_symbol, rules, nonterm_userdef, term_userdef, diction, firsts, follows\n",
    "\n",
    "    # The solset set will store the symbols in the FOLLOW set for the given non-terminal.\n",
    "    solset = set()\n",
    "\n",
    "    # Handling the Start Symbol:\n",
    "    if nt == start_symbol:\n",
    "        solset.add('$')\n",
    "    \n",
    "    # Iterating Over Non-terminals and Production Rules:\n",
    "    for curNT in diction:\n",
    "        rhs = diction[curNT]\n",
    "        for subrule in rhs:\n",
    "            # Finding the Occurrences of the Target Non-terminal in a Rule:\n",
    "            if nt in subrule:\n",
    "                while nt in subrule:\n",
    "                    index_nt = subrule.index(nt)\n",
    "                    subrule = subrule[index_nt + 1:]\n",
    "                    # Handling Symbols Following the Target Non-terminal:\n",
    "                    if len(subrule) != 0:\n",
    "                     res = first(subrule)\n",
    "                     # Handling Epsilon Transitions in FIRST set\n",
    "                     if '#' in res:\n",
    "                        newList = []\n",
    "                        res.remove('#')\n",
    "                        ansNew = follow(curNT)\n",
    "                        if ansNew != None:\n",
    "                            if type(ansNew) is list:\n",
    "                                newList = res + ansNew\n",
    "                            else:\n",
    "                                newList = res + [ansNew]\n",
    "                        else:\n",
    "                            newList = res\n",
    "                        res = newList\n",
    "\n",
    "                    else:\n",
    "                     if nt != curNT:\n",
    "                        res = follow(curNT)\n",
    "\n",
    "                    # Adding Symbols to solset\n",
    "                    if res is not None:\n",
    "                            if type(res) is list:\n",
    "                                for g in res:\n",
    "                                    solset.add(g)\n",
    "                            else:\n",
    "                                solset.add(res)\n",
    "\n",
    "    return list(solset)\n",
    "\n",
    "# ===================================================================================================\n",
    "\n",
    "def computeAllFirsts():\n",
    "    global rules, nonterm_userdef, term_userdef, diction, firsts\n",
    "    for rule in rules:\n",
    "        k = rule.split(\"->\")\n",
    "        k[0] = k[0].strip()\n",
    "        k[1] = k[1].strip()\n",
    "        rhs = k[1]\n",
    "        multirhs = rhs.split('|')\n",
    "        for i in range(len(multirhs)):\n",
    "            multirhs[i] = multirhs[i].strip()\n",
    "            multirhs[i] = multirhs[i].split()\n",
    "        diction[k[0]] = multirhs\n",
    "    # Open a file named rules.txt in write mode\n",
    "    with open('rules.txt', 'w') as file:\n",
    "        file.write(\"Rules:\\n\")\n",
    "        for y in diction:\n",
    "            file.write(f\"{y} -> {diction[y]}\\n\") \n",
    "\n",
    "    # Remove left recursion\n",
    "    diction = removeLeftRecursion(diction)\n",
    "    # Remove left factoring\n",
    "    diction = LeftFactoring(diction)\n",
    "    for y in list(diction.keys()):\n",
    "        t = set()\n",
    "        for sub in diction.get(y):\n",
    "            res = first(sub)\n",
    "            if res != None:\n",
    "                if type(res) is list:\n",
    "                    for u in res:\n",
    "                        t.add(u)\n",
    "                else:\n",
    "                    t.add(res)\n",
    "        firsts[y] = t\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"\\nCalculated firsts: \")\n",
    "    key_list = list(firsts.keys())\n",
    "    index = 0\n",
    "    for gg in firsts:\n",
    "     print(f\"first({key_list[index]}) \"f\"=> {firsts.get(gg)}\")\n",
    "     index += 1   \n",
    "\n",
    "# ==================================================================================================== \n",
    "\n",
    "def computeAllFollows():\n",
    "    global start_symbol, rules, nonterm_userdef, term_userdef, diction, firsts, follows\n",
    "    for NT in diction:\n",
    "        solset = set()\n",
    "        sol = follow(NT)\n",
    "        if sol is not None:\n",
    "            for g in sol:\n",
    "                solset.add(g)\n",
    "        follows[NT] = solset\n",
    "    print(\"\\nCalculated follows: \")\n",
    "    key_list = list(follows.keys())\n",
    "    index = 0\n",
    "    for gg in follows:\n",
    "     print(f\"follow({key_list[index]})\"f\" => {follows[gg]}\")\n",
    "     index += 1 \n",
    "\n",
    "# ===================================================================================================   \n",
    "\n",
    "def createParseTable():\n",
    "    global diction, firsts, follows, term_userdef\n",
    "    print(\"\\n\")\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"Firsts and Follow Result table\")\n",
    "    # Printing FIRST and FOLLOW Sets\n",
    "    mx_len_first = 0\n",
    "    mx_len_fol = 0\n",
    "    for u in diction:\n",
    "        k1 = len(str(firsts[u]))\n",
    "        k2 = len(str(follows[u]))\n",
    "        if k1 > mx_len_first:\n",
    "            mx_len_first = k1\n",
    "        if k2 > mx_len_fol:\n",
    "            mx_len_fol = k2\n",
    "    print(tabulate([[\"Non-T\", \"FIRST\", \"FOLLOW\"]] + [[u, str(firsts[u]), str(follows[u])] for u in diction], headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    ntlist = list(diction.keys())\n",
    "    terminals = copy.deepcopy(term_userdef)\n",
    "    terminals.remove('(')\n",
    "    terminals.remove(')')\n",
    "    terminals.remove('+')\n",
    "    terminals.remove('-')\n",
    "    terminals.remove('=')\n",
    "    terminals.remove('1')\n",
    "    terminals.append('$')\n",
    "    mat = []\n",
    "    for x in diction:\n",
    "        row = []\n",
    "        for y in terminals:\n",
    "            row.append('')\n",
    "        mat.append(row)\n",
    "    grammar_is_LL = True\n",
    "\n",
    "    # Filling in the Parsing Table:\n",
    "    for lhs in diction:\n",
    "        rhs = diction[lhs]\n",
    "        for y in rhs:\n",
    "            res = first(y)\n",
    "            if '#' in res:\n",
    "                if type(res) == str:\n",
    "                    firstFollow = []\n",
    "                    fol_op = follows[lhs]\n",
    "                    if fol_op is str:\n",
    "                        firstFollow.append(fol_op)\n",
    "                    else:\n",
    "                        for u in fol_op:\n",
    "                            firstFollow.append(u)\n",
    "                    res = firstFollow\n",
    "                else:\n",
    "                    res.remove('#')\n",
    "                    res = list(res) + list(follows[lhs])\n",
    "            ttemp = []\n",
    "            if type(res) is str:\n",
    "                ttemp.append(res)\n",
    "                res = copy.deepcopy(ttemp)\n",
    "            for c in res:\n",
    "                xnt = ntlist.index(lhs)\n",
    "                yt = terminals.index(c)\n",
    "                if mat[xnt][yt] == '':\n",
    "                    mat[xnt][yt] = f\"{lhs}->{' '.join(y)}\"\n",
    "                else:\n",
    "                    if f\"{lhs}->{y}\" in mat[xnt][yt]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        grammar_is_LL = False\n",
    "                        mat[xnt][yt] = mat[xnt][yt] \\\n",
    "                            + f\",{lhs}->{' '.join(y)}\"\n",
    "\n",
    "    with open('parsingtable.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Generated parsing table:\\n\")\n",
    "        headers = [\"\"] + terminals\n",
    "        rows = [[ntlist[j]] + y for j, y in enumerate(mat)]\n",
    "        file.write(tabulate(rows, headers, tablefmt='fancy_grid'))\n",
    "\n",
    "    return (mat, grammar_is_LL, terminals)\n",
    "\n",
    "# =================================================================================================\n",
    "\n",
    "def validateStringUsingStackBuffer(parsing_table, grammarll1, table_term_list, input_string, term_userdef, start_symbol):\n",
    "    with open('parsing.txt', 'w') as file:\n",
    "        # file.write(f\"Validate String:\\n{input_string}\\n\")\n",
    "        print(f\"Validate String:\\n\\n{input_string}\\n\")\n",
    "        print(\"Look into the parsing.txt file for the parsing steps\")\n",
    "        print(\"\\n\")\n",
    "        if grammarll1 == False:\n",
    "            file.write(f\"Input String = \\\"{input_string}\\\"\\nGrammar is not LL(1)\\n\")\n",
    "        stack = [start_symbol, '$']\n",
    "        buffer = []\n",
    "        input_string = input_string.split()\n",
    "        input_string.reverse()\n",
    "        buffer = ['$'] + input_string\n",
    "\n",
    "        # Writing Header for Parsing Steps:\n",
    "        file.write(\"{:>70} {:>10} {:>20}\\n\".format(\"Input\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", \"Stack\\t\\t\", \"Action\"))\n",
    "        while True:\n",
    "            # Checking for Valid End Condition:\n",
    "            if stack == ['$'] and buffer == ['$']:\n",
    "                file.write(\"{:>100} | {:>25} | {:>30}\\n\".format(' '.join(buffer), ' '.join(stack), \"Valid\"))\n",
    "                return \"Valid String!\"\n",
    "            \n",
    "            # Parsing Non-terminals:\n",
    "            elif stack[0] not in term_userdef:\n",
    "                x = list(diction.keys()).index(stack[0])\n",
    "                y = table_term_list.index(buffer[-1])\n",
    "                if parsing_table[x][y] != '':\n",
    "                    entry = parsing_table[x][y]\n",
    "                    file.write(\"{:>100} | {:>25} | {:>30}\\n\".format(' '.join(buffer), ' '.join(stack), f\"T[{stack[0]}][{buffer[-1]}] = {entry}\"))\n",
    "                    lhs_rhs = entry.split(\"->\")\n",
    "                    lhs_rhs[1] = lhs_rhs[1].replace('#', '').strip()\n",
    "                    entryrhs = lhs_rhs[1].split()\n",
    "                    stack = entryrhs + stack[1:]\n",
    "                else:\n",
    "                    file.write(f\"Invalid String! No rule at Table[{stack[0]}][{buffer[-1]}]\\n\")\n",
    "                    return \"Invalid String! No rule at Table[{stack[0]}][{buffer[-1]}]\"\n",
    "                \n",
    "            # Matching Terminals:\n",
    "            else:\n",
    "                if stack[0] == buffer[-1]:\n",
    "                    file.write(\"{:>100} | {:>25} | {:>30}\\n\".format(' '.join(buffer), ' '.join(stack), f\"Matched1:{stack[0]}\"))\n",
    "                    buffer = buffer[:-1]\n",
    "                    stack = stack[1:]\n",
    "                else:\n",
    "                    file.write(\"Invalid String! Unmatched terminal symbols\\n\")\n",
    "                    return \"Invalid String! Unmatched terminal symbols\"\n",
    "\n",
    "# ==============================================================================================\n",
    "sample_input_string = None\n",
    "inps = ''\n",
    "with open('input.txt', 'r+') as f:\n",
    "    for line in f.readlines():\n",
    "        inps += line\n",
    "sample_input_string = inps\n",
    "arr1 = inps.split()[4] # L[10]\n",
    "id = inps.split()[7] # maxval\n",
    "arr2 = inps.split()[9] # L[0]\n",
    "arr3 = inps.split()[21] # L[i]\n",
    "\n",
    "# Prints all the tokens\n",
    "print(\"The tokens are:\\n\")\n",
    "print(inps.split())\n",
    "with open('tokens.txt','w') as f:\n",
    "    for token in inps.split():\n",
    "        f.write(token+\"\\n\")\n",
    "\n",
    "# Checking if the identifier maxval is following the format of a valid identifier\n",
    "x = re.search(\"^[a-zA-Z][a-zA-Z0-9_]*\", id)\n",
    "if not x:\n",
    "    print(\"Invalid identifier\")\n",
    "    exit(1)\n",
    "\n",
    "# Checking if L[10] is a valid array name or not\n",
    "y = re.search(\"^[a-zA-Z][a-zA-Z0-9_]*[[0-9]+]\", arr1)\n",
    "if not y:\n",
    "    print(\"Invalid array name\")\n",
    "    exit(1)\n",
    "\n",
    "# Checking if L[0] is a valid array name or not\n",
    "z = re.search(\"^[a-zA-Z][a-zA-Z0-9_]*[[0-9]+]\", arr2)\n",
    "if not z:\n",
    "    print(\"Invalid initialization\")\n",
    "    exit(1)\n",
    "\n",
    "# Checking if L[i] is a valid array name or not\n",
    "zz = re.search(\"^[a-zA-Z][a-zA-Z0-9_]*[[a-z]+]\", arr3)\n",
    "if not zz:\n",
    "    print(\"Invalid index\")\n",
    "    exit(1)\n",
    "\n",
    "# Rules for LL(1) grammar\n",
    "rules = [\n",
    "    \"S -> T M B A D\",\n",
    "    \"T -> int\",\n",
    "    \"M -> main()\",\n",
    "    \"B -> begin\",\n",
    "    \"D -> End\",\n",
    "    \"A -> E F G W X\",\n",
    "    \"E -> T \" + arr1 + \" ;\",\n",
    "    \"F -> T \" + id + \" = \" + arr2 + \" ;\",\n",
    "    \"G -> for C do\",\n",
    "    \"C -> i = 1 to n - 1\",\n",
    "    \"W -> if \" + arr3 + \" > \" + id + \" P Q R\",\n",
    "    \"P -> \" + id + \" = \" + arr3 + \" ;\",\n",
    "    \"Q -> endif\",\n",
    "    \"R -> endfor\",\n",
    "    \"X -> return ( \" + id + \" )\"\n",
    "]\n",
    "\n",
    "# List of non terminals in our grammar\n",
    "nonterm_userdef = ['S', 'T', 'M', 'B', 'D', 'A', 'E', 'F', 'K', 'Z', 'G', 'W', 'P', 'Q', 'R', 'X', 'C']\n",
    "\n",
    "# List of terminals in our grammar\n",
    "term_userdef = [id, arr1, arr2, arr3, 'n', 'int', 'main()', 'End', 'for', 'if', 'begin', 'do', 'i', 'to', '(', ')', '+', '-', 'endif', 'endfor', 'return', '>', '=', '1', ',', ';']\n",
    "\n",
    "# \"diction\" dictionary is used to store the production rules of a context-free grammar.\n",
    "diction = {}\n",
    "\n",
    "# Dictionary to store the firsts of the productions\n",
    "firsts = {}\n",
    "\n",
    "# Dictionary to store the follows of the productions\n",
    "follows = {}\n",
    "\n",
    "# Call the function to compute all the firsts of the productions\n",
    "computeAllFirsts()\n",
    "\n",
    "# Extracting the stating symbol\n",
    "start_symbol = list(diction.keys())[0]\n",
    "\n",
    "# Call the function to compute the follows of all the productions\n",
    "computeAllFollows()\n",
    "\n",
    "# Call the function to create a parsing table\n",
    "(parsing_table, result, tabTerm) = createParseTable()\n",
    "\n",
    "# Check if the i/p string is not empty\n",
    "if sample_input_string != None:\n",
    "  validity = validateStringUsingStackBuffer(parsing_table, result,\n",
    "  tabTerm, sample_input_string,term_userdef, start_symbol)\n",
    "  print(validity)\n",
    "else:\n",
    "    print(\"No input String detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242297b-dc38-45d9-9494-948c9d3a6380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
